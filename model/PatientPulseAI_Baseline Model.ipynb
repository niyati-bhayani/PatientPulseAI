{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a4dd91b-7c82-4222-afce-02760820b18f",
   "metadata": {},
   "source": [
    "<b> Import Libraries </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05bbe44d-2e6e-47ae-948e-2e0a9da2948e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59116664-efd4-431e-894e-669d6adffb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch #PyTorch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset #Used to load pre-defined datasets (e.g., IMDB) from the Hugging Face datasets library\n",
    "import evaluate #Calculate metrics like accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a011612f-1ac5-4c0f-a8d3-9d2f636ca053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IMDB dataset\n",
    "imdb_data = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08ab2b8-b3c8-49b8-a7f8-b80a493a6038",
   "metadata": {},
   "source": [
    "<b>IMDB Dataset Overview</b>:\n",
    "\n",
    "- The IMDB dataset contains 50,000 movie reviews, with each review labeled as positive (1) or negative (0).\n",
    "\n",
    "- It's split into a training set (25,000 examples) and a test set (25,000 examples).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd467112-fbfc-4c42-ab4d-357438101841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the dataset structure\n",
    "print(imdb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1ca232-8fab-40a2-8a51-aca8c7ee5765",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample of the training data\n",
    "print(imdb_data['train'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698b2910-ad01-49fa-b83e-85f2f2d6c7f4",
   "metadata": {},
   "source": [
    "<b> BERT Tokenizer and Model </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acea7d5c-3ea9-49c0-bfd6-660810016dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') #Loads a pre-trained BERT tokenizer (bert-base-uncased), which splits text into tokens compatible with the BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbae93f8-f65b-4948-88e7-f59edd8e8c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads a pre-trained BERT model for binary classification (num_labels=2)\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283893c9-b46b-4c3d-be9a-df8fcce351d5",
   "metadata": {},
   "source": [
    "<b> Start of Example Code. Ignore when working on the real data. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9130d47-fa47-4879-b37d-adb7b97fac99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example to understand the model better\n",
    "text = \"The movie was absolutely fantastic, with brilliant performances!\"\n",
    "label = torch.tensor([1])  # Positive sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ff989f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') #Loads a pre-trained BERT tokenizer (bert-base-uncased), which splits text into tokens compatible with the BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9c4cd6f-323d-43fc-93c8-de484d7884ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1996,  3185,  2001,  7078, 10392,  1010,  2007,  8235,  4616,\n",
       "           999,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(text, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144db1ae-822e-40c4-9eba-9cd0528ff98c",
   "metadata": {},
   "source": [
    "input_ids: Tokenized IDs of the text, including special tokens:\n",
    "- [101] = [CLS] (start of the sentence)\n",
    "- [102] = [SEP] (end of the sentence)\n",
    "\n",
    "attention_mask: Indicates which tokens are real (1) and which are padding (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fcd6ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#Loads a pre-trained BERT model for binary classification (num_labels=2)\n",
    "model1 = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adacfc47-a152-4de4-8319-27e04344990a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(1.2709, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3429, -0.5987]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model1(**inputs,labels=label)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b1606d-ddc5-4f48-b15c-c4386725b520",
   "metadata": {},
   "source": [
    "bert-base-uncased:\n",
    "\n",
    "- Pre-trained BERT model with 12 transformer layers.\n",
    "- Processes input_ids and computes contextual embeddings for each token.\n",
    "- The [CLS] token’s embedding is used as the representation of the entire input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7553db4-54c2-4fe1-a7b5-31e12e1c9590",
   "metadata": {},
   "source": [
    "Classification Head:\n",
    "\n",
    "- A fully connected layer is applied to the [CLS] token’s embedding.\n",
    "- Outputs logits: raw scores for each class (positive and negative sentiment).\n",
    "- [ 0.1753, -0.1100 ]: Higher score for the first class (0 → negative sentiment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f2ca9fb-2673-4741-a28f-5c12e3f237e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7194, 0.2806]], grad_fn=<SoftmaxBackward0>)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "predictions = torch.softmax(outputs.logits, dim=1)\n",
    "print(predictions)\n",
    "predicted_label = torch.argmax(predictions, dim=1).item()\n",
    "print(predicted_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91dd8dc-ec8d-4ec3-948c-0b06682919b9",
   "metadata": {},
   "source": [
    "- 0.5708 --> Probability of Class 0 (negative class)\n",
    "- 0.4292 --> Probability of Class 1 (positive class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2372d3f8-0493-4fba-99fd-c920a175c1dd",
   "metadata": {},
   "source": [
    "- 0 : Predicted Label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7232fab-5a90-4efa-b339-ffa096cfd64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configures the optimizer with model parameters and a learning rate of 2e-5\n",
    "optimizer = AdamW(model1.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ad1cf21-9508-44a2-a701-a9952789ad25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2709, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss Calculation: During training, the model compares logits with the ground truth (label = 1) using cross-entropy loss.\n",
    "loss = outputs.loss\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "517a3a04-3575-415a-833e-f12f7e4b7117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation: Compute gradients and update model weights\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01e2f509-33c6-42f4-ad75-c1a521f26480",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31447f20-cfc4-4fdc-a9c0-c1413563486f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.2709351778030396\n",
      "Logits: tensor([[ 0.3429, -0.5987]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print(f\"Loss: {loss.item()}\")\n",
    "print(f\"Logits: {logits}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069a10b2-875e-4ebe-b681-87a186fbb831",
   "metadata": {},
   "source": [
    "For evaluation:\n",
    "- Use the trained model to predict sentiments for test data.\n",
    "- Compare predictions with ground truth labels and compute metrics like accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eae39737-c586-466f-a0d8-cf2bc8063fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use torch.argmax to get the class with the highest probability\n",
    "predictions = torch.argmax(logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1390b17-5dc1-4083-b4f2-570133531e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detach tensors and convert them to lists/NumPy arrays\n",
    "predictions = predictions.detach().cpu().numpy()\n",
    "references = label.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53765803-82eb-4855-9317-08a718213d21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.0}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "accuracy = accuracy_metric.compute(predictions=predictions, references=references)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c60689-f059-460e-a12f-b302e99f030e",
   "metadata": {},
   "source": [
    "<b> End of Example Code </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f57096-e3a2-4d80-98ae-2a4e1f9f998b",
   "metadata": {},
   "source": [
    "<b> Custom Dataset Class </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa2674c-4270-4c78-84d9-cbe51e5c7fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDbDataset(Dataset):\n",
    "    #Initializes the dataset with tokenized encodings and corresponding labels\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    #Returns the number of examples in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    #Retrieves a single example at a given index as a dictionary containing: input_ids, attention_mask, and labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cf23a4-1a0e-45cb-a6f5-6802831cbcf7",
   "metadata": {},
   "source": [
    "<b> Tokenization </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a99cac1-1cb1-48a5-923e-4bdcddbe6657",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracts text and labels for training and testing splits\n",
    "train_texts = imdb_data['train']['text']\n",
    "train_labels = imdb_data['train']['label']\n",
    "test_texts = imdb_data['test']['text']\n",
    "test_labels = imdb_data['test']['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576b4f5b-0968-408f-bf9f-a305dd4138e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizes the text data\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128) \n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707df8b8-527e-4998-8a19-d2c3cf7b97e9",
   "metadata": {},
   "source": [
    "truncation=True: Truncates text longer than 128 tokens\n",
    "\n",
    "padding=True: Pads shorter text to 128 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0663217a-dfd6-4c3b-9d0b-68ef18f7a29d",
   "metadata": {},
   "source": [
    "<b> Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00a55f3-de18-4791-8264-0e187e14a74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates IMDbDataset objects for training and testing data\n",
    "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "test_dataset = IMDbDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3a6134-c352-4a0e-b184-967ab641d0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wraps datasets in DataLoader for batch processing\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f674dfc-40fd-4bcf-bcc9-14eb53b185b8",
   "metadata": {},
   "source": [
    "batch_size=16: Each batch contains 16 examples\n",
    "\n",
    "shuffle=True: Shuffles training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b907f697-65c5-42a7-8b06-c37bf680c10f",
   "metadata": {},
   "source": [
    "<b> Optimizer </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dc36fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configures the optimizer with model parameters and a learning rate of 2e-5\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f996df-72fd-4c33-8d54-0ce9760ee077",
   "metadata": {},
   "source": [
    "<b> Device Configuration </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1757b409-2b74-434a-99dd-24ac4dcb30e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Moves the model to GPU (cuda) if available; otherwise, uses CPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d685013c-4caf-476e-8a5e-608e90128f4b",
   "metadata": {},
   "source": [
    "<b> Training Loop </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26c6080-3b14-4a83-b421-bf65f3f05c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trains the model for 3 epochs\n",
    "for epoch in range(3): \n",
    "    model.train()\n",
    "    total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c79e7f-90b3-4405-992d-6fbc48c77f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    total_loss += loss.item()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88b52e3-a01d-405b-ac40-068b6ac43c6c",
   "metadata": {},
   "source": [
    "For each batch:\n",
    "\n",
    "- Clears gradients: optimizer.zero_grad()\n",
    "\n",
    "- Processes inputs: input_ids, attention_mask, and labels\n",
    "\n",
    "- Computes loss\n",
    "\n",
    "- Backpropagates gradients: loss.backward()\n",
    "\n",
    "- Updates model parameters: optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fd8f9a-05f5-42d0-9252-6422187b2fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prints the average loss after each epoch\n",
    "print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0259a5c2-eb7f-47dd-b017-39a4c2bc599a",
   "metadata": {},
   "source": [
    "<b> Evaluation </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dade8d3-bbfe-4190-ad9d-b3c4d03ab987",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads the accuracy matrix\n",
    "accuracy_metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed764425-238b-4b30-ae51-89e596aea279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sets the model to evaluation mode and initializes storage for predictions and labels\n",
    "model.eval()\n",
    "predictions = []\n",
    "references = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d19cde2-b93c-49fc-8797-34d46906f9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test_loader:\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    preds = torch.argmax(outputs.logits, dim=-1)\n",
    "    predictions.extend(preds.cpu().numpy())\n",
    "    references.extend(labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4800b9-20c1-4a81-abc7-a3d25c417feb",
   "metadata": {},
   "source": [
    "For each batch in the test set:\n",
    "\n",
    "- Moves inputs to the appropriate device.\n",
    "\n",
    "- Predicts logits without computing gradients: torch.no_grad().\n",
    "\n",
    "- Converts logits to predictions: torch.argmax().\n",
    "\n",
    "- Stores predictions and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fea969-3a98-45a5-853c-3a1a394bf2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computes and prints the test set accuracy\n",
    "accuracy = accuracy_metric.compute(predictions=predictions, references=references)\n",
    "print(f\"Test Accuracy: {accuracy['accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783daaf2-6f05-44d1-bc3f-b5a05be796b4",
   "metadata": {},
   "source": [
    "<b> Save Model </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3decbbe4-8677-47fa-a8fe-5ddb7e744f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('sentiment_model')\n",
    "tokenizer.save_pretrained('sentiment_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
